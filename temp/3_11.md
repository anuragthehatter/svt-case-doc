Slide 1

There are 3 CNS nodes in Scale-lab cluster. Every node is with a 894G NVMe device
 for glusterfs.

The instance type for those CNS nodes is 4xlarge, which has 16 CPUs and 65G of memory.

We use the 3.4.0 version of CNS images in the test. Those are the latest CNS images
at the time of the test. The next version 3.11 was not available at that time.

Heketi pod and block-provisioner pod run on Infra nodes.

Slide 2

The goal of the test is to see the performance of CNS under the IO workload
 generated by the applications shipped with OCP templates.

In this part we focus on Jenkins, Redis, JBossAMQ, and gitWorkload.

For each application, we created a number of projects on cluster. Each of them
contains an application pod and then we start to generate the IO workload
on those pods.

The table shows the system stats from pbench on one of the CNS nodes.
The first row is the number of projects for each app that we reached in the test.
From the IO perspective, AMQ and Git created the most stress on CNS nodes.

I do not have the time to cover the tests for each application.
Some of the results is in the appendix of this presentation sildes.
More details are discussed in the link of the previous slide.

Slide 3

CNS can support over 100 projects for the applications in the test.
Gluster-block is for Jenkins and Git while Gluster-file is for the Redis and AMQ.
The OCP template for AMQ requires ReadWriteMany access mode and Gluster-block does not
support it.

In the test, the benchmark tool also generated the measurements like failing rate, throughput or
execution time. It would be great if experts on those application can take a look
at those numbers. Tuning on the application parameters might be helpful to achieve better
performance.

The CNS PVC provisioning and deleting still have issues. In the test, the IO
 workload lasts usually about 2 hours but PVC creating and cleaning-up can take much
 more than that. In some cases, we have to reinstall CNS to proceed for the next application.
 We have bugs open for those issues. Those are test automation blockers.

The last bullet on this slide provides the links of the pbench-data on the server.
We can use them as baseline to compare for the next release.
